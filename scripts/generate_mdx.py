#!/usr/bin/env python3
"""
MDX Generator for Docusaurus

Converts curriculum markdown modules to MDX format with React components.

Usage:
    python scripts/generate_mdx.py [lang_pair] [level] [module_num] [--validate]

Examples:
    python scripts/generate_mdx.py l2-uk-en              # All levels
    python scripts/generate_mdx.py l2-uk-en a1           # All A1 modules
    python scripts/generate_mdx.py l2-uk-en a1 5         # Module 5 only
    python scripts/generate_mdx.py l2-uk-en --validate   # Generate + validate
"""

import re
import sys
import json
import yaml
from pathlib import Path
from dataclasses import dataclass
from typing import Optional

# Add current dir to path for imports
SCRIPT_DIR = Path(__file__).parent
sys.path.append(str(SCRIPT_DIR))
from yaml_activities import ActivityParser, Activity
from manifest_utils import get_module_by_slug

# Paths
PROJECT_ROOT = SCRIPT_DIR.parent
CURRICULUM_DIR = PROJECT_ROOT / "curriculum"
DOCUSAURUS_DIR = PROJECT_ROOT / "docusaurus" / "docs"

def dump_json_for_jsx(data):
    """Dump JSON string escaped for use inside a JSX template literal."""
    s = json.dumps(data, ensure_ascii=False)
    # Escape backslashes first to avoid double escaping other chars
    s = s.replace('\\', '\\\\')
    # Escape backticks for template literals
    s = s.replace('`', '\\`')
    # Escape $ to avoid template interpolation
    s = s.replace('${', '\\${')
    return s

# =============================================================================
# DATA CLASSES
# =============================================================================

@dataclass
class QuizQuestion:
    question: str
    options: list[dict]  # [{"text": str, "correct": bool}]

@dataclass
class MatchPair:
    left: str
    right: str

@dataclass
class FillInItem:
    sentence: str
    answer: str
    options: list[str]

@dataclass
class TrueFalseItem:
    statement: str
    is_true: bool
    explanation: str

@dataclass
class UnjumbleItem:
    jumbled: str
    answer: str

@dataclass
class GroupSortData:
    groups: dict[str, list[str]]  # {group_name: [items]}

@dataclass
class AnagramItem:
    scrambled: str
    answer: str
    hint: str

@dataclass
class ErrorCorrectionItem:
    sentence: str
    errorWord: str
    correctForm: str
    options: list[str]
    explanation: str

@dataclass
class ClozeData:
    passage: str
    blanks: list[dict]  # [{"answer": str, "options": list[str]}]

@dataclass
class SelectQuestion:
    question: str
    options: list[dict]  # [{"text": str, "correct": bool}]

@dataclass
class TranslateQuestion:
    source: str
    options: list[dict]  # [{"text": str, "correct": bool}]

@dataclass
class MarkTheWordsItem:
    text: str  # Plain text with marks removed
    correctWords: list[str]  # List of correct words to mark

@dataclass
class MorphemeItem:
    word: str  # The full word containing the morpheme
    morpheme: str  # The morpheme to highlight within the word
    type: str = 'unknown'  # prefix, root, or suffix

@dataclass
class HighlightMorphemesItem:
    text: str  # Plain text with asterisks removed
    morphemes: list[MorphemeItem]  # List of morphemes to highlight
    instruction: str = ''  # Optional instruction text

@dataclass
class EssayResponseData:
    prompt: str
    modelAnswer: str
    rubric: str

@dataclass
class ComparativeStudyData:
    content: str
    task: str
    modelAnswer: str

# =============================================================================
# UTILITIES
# =============================================================================

def escape_jsx(text: str) -> str:
    """Escape text for use in JSX strings (both template literals and double quotes).
    
    Uses HTML entity &quot; for double quotes to avoid JSX parsing errors.
    See issue #396 for details.
    """
    if not text:
        return ''
    # Convert to string if not already (handles int/float from YAML)
    if not isinstance(text, str):
        text = str(text)
    # Escape backslashes first, then other special chars
    text = text.replace('\\', '\\\\')
    text = text.replace('`', '\\`')
    text = text.replace('"', '&quot;')  # HTML entity, not backslash escape
    text = text.replace('${', '\\${')
    return text

def fix_html_for_jsx(text: str) -> str:
    """Convert HTML tags to JSX-compatible self-closing format."""
    # Convert <br> to <br />
    text = re.sub(r'<br\s*/?>', '<br />', text)
    # Convert <hr> to <hr />
    text = re.sub(r'<hr\s*/?>', '<hr />', text)
    # Convert <img ...> to <img ... />
    text = re.sub(r'<img([^>]*?)(?<!/)>', r'<img\1 />', text)
    return text

def parse_frontmatter(content: str) -> tuple[dict, str]:
    """Extract YAML frontmatter and body from markdown."""
    lines = content.split('\n')
    if not lines or lines[0] != '---':
        return {}, content

    end_idx = -1
    for i, line in enumerate(lines[1:], 1):
        if line == '---':
            end_idx = i
            break

    if end_idx == -1:
        return {}, content

    yaml_content = '\n'.join(lines[1:end_idx])
    body = '\n'.join(lines[end_idx + 1:])

    try:
        fm = yaml.safe_load(yaml_content) or {}
    except yaml.YAMLError:
        fm = {}

    return fm, body

# =============================================================================
# YAML ACTIVITY SUPPORT
# =============================================================================

def yaml_activities_to_jsx(activities: list[Activity], is_ukrainian_forced: bool = False) -> str:
    """Convert YAML activities to JSX components using the shared ActivityParser."""
    parser = ActivityParser()
    return parser.to_mdx(activities, is_ukrainian_forced)

# =============================================================================
# ACTIVITY PARSERS
# =============================================================================

def parse_quiz(content: str) -> list[QuizQuestion]:
    """Parse quiz activity with numbered questions or --- separated questions."""
    questions = []

    # Check if content has numbered format (1. question)
    has_numbered = bool(re.search(r'^\d+\.\s', content, flags=re.MULTILINE))

    if has_numbered:
        # Split by numbered items
        blocks = re.split(r'(?=^\d+\.\s)', content, flags=re.MULTILINE)
        for block in blocks:
            block = block.strip()
            if not block or not re.match(r'^\d+\.', block):
                continue
            block = re.sub(r'^\d+\.\s*', '', block)
            q = _parse_quiz_block(block)
            questions.extend(q)
    else:
        # Split by --- separators (horizontal rules)
        blocks = re.split(r'\n---+\n', content)
        for block in blocks:
            block = block.strip()
            if not block:
                continue
            q = _parse_quiz_block(block)
            questions.extend(q)

    return questions

def _parse_quiz_block(block: str) -> list[QuizQuestion]:
    """Parse a single quiz question block."""
    lines = block.strip().split('\n')
    question = ''
    options = []

    for line in lines:
        stripped = line.strip()
        # Check for option lines
        if stripped.startswith('- [x]'):
            options.append({"text": stripped[5:].strip(), "correct": True})
        elif stripped.startswith('- [ ]'):
            options.append({"text": stripped[5:].strip(), "correct": False})
        elif not stripped.startswith('-'):
            if not options:  # Still building question
                question = (question + ' ' + stripped).strip()

    if options:
        return [QuizQuestion(question=question, options=options)]
    return []

def parse_match_up(content: str) -> list[MatchPair]:
    """Parse match-up activity with :: separator or table format."""
    pairs = []

    # Try :: separator format first
    for line in content.split('\n'):
        line = line.strip()
        if '::' in line:
            if line.startswith('- '):
                line = line[2:]
            parts = line.split('::', 1)
            if len(parts) == 2:
                pairs.append(MatchPair(left=parts[0].strip(), right=parts[1].strip()))

    if pairs:
        return pairs

    # Try table format (skip header row and separator)
    header_skipped = False
    for line in content.split('\n'):
        line = line.strip()
        if line.startswith('|') and '|' in line[1:]:
            cells = [c.strip() for c in line.split('|')[1:-1]]
            # Skip separator line (|---|---|)
            if cells and cells[0].startswith('-'):
                continue
            # Skip header row (first non-separator row)
            if not header_skipped:
                header_skipped = True
                continue
            if len(cells) >= 2 and cells[0] and cells[1]:
                pairs.append(MatchPair(left=cells[0], right=cells[1]))

    return pairs

def parse_fill_in(content: str) -> list[FillInItem]:
    """Parse fill-in activity with answer and options callouts."""
    items = []
    # Split by numbered items
    blocks = re.split(r'^\d+\.\s+', content, flags=re.MULTILINE)

    for block in blocks:
        if not block.strip():
            continue

        lines = block.split('\n')
        sentence = lines[0].strip() if lines else ''
        answer = ''
        options = []

        for line in lines[1:]:
            stripped = line.strip()
            if stripped.startswith('> [!answer]'):
                answer = stripped.replace('> [!answer]', '').strip()
            elif stripped.startswith('> [!options]'):
                opts_str = stripped.replace('> [!options]', '').strip()
                options = [o.strip() for o in opts_str.split('|')]

        if sentence and answer:
            items.append(FillInItem(sentence=sentence, answer=answer, options=options))

    return items

def parse_true_false(content: str) -> list[TrueFalseItem]:
    """Parse true-false activity with checkbox format."""
    items = []
    lines = content.split('\n')

    i = 0
    while i < len(lines):
        line = lines[i].strip()

        if line.startswith('- [x]'):
            statement = line[5:].strip()
            explanation = ''
            # Check for explanation on next line
            if i + 1 < len(lines) and lines[i + 1].strip().startswith('>'):
                explanation = lines[i + 1].strip()[1:].strip()
                i += 1
            items.append(TrueFalseItem(statement=statement, is_true=True, explanation=explanation))
        elif line.startswith('- [ ]'):
            statement = line[5:].strip()
            explanation = ''
            if i + 1 < len(lines) and lines[i + 1].strip().startswith('>'):
                explanation = lines[i + 1].strip()[1:].strip()
                i += 1
            items.append(TrueFalseItem(statement=statement, is_true=False, explanation=explanation))

        i += 1

    return items

def parse_unjumble(content: str) -> list[UnjumbleItem]:
    """Parse unjumble activity with answer callouts."""
    items = []
    blocks = re.split(r'^\d+\.\s+', content, flags=re.MULTILINE)

    for block in blocks:
        if not block.strip():
            continue

        lines = block.split('\n')
        jumbled = lines[0].strip() if lines else ''
        answer = ''

        for line in lines[1:]:
            stripped = line.strip()
            if stripped.startswith('> [!answer]'):
                answer = stripped.replace('> [!answer]', '').strip()
                break

        if jumbled and answer:
            items.append(UnjumbleItem(jumbled=jumbled, answer=answer))

    return items

def parse_group_sort(content: str) -> GroupSortData:
    """Parse group-sort activity with ### group headers."""
    groups = {}
    current_group = None

    for line in content.split('\n'):
        stripped = line.strip()

        if stripped.startswith('### '):
            current_group = stripped[4:].strip()
            groups[current_group] = []
        elif stripped.startswith('- ') and current_group:
            groups[current_group].append(stripped[2:].strip())

    return GroupSortData(groups=groups)

def parse_anagram(content: str) -> list[AnagramItem]:
    """Parse anagram activity."""
    items = []
    blocks = re.split(r'^\d+\.\s+', content, flags=re.MULTILINE)

    for block in blocks:
        if not block.strip():
            continue

        lines = block.split('\n')
        scrambled = lines[0].strip() if lines else ''
        answer = ''
        hint = ''

        for line in lines[1:]:
            stripped = line.strip()
            if stripped.startswith('> [!answer]'):
                answer = stripped.replace('> [!answer]', '').strip()
            elif stripped.startswith('> [!hint]'):
                hint = stripped.replace('> [!hint]', '').strip()

        if scrambled and answer:
            items.append(AnagramItem(scrambled=scrambled, answer=answer, hint=hint))

    return items

def parse_error_correction(content: str) -> list[ErrorCorrectionItem]:
    """Parse error-correction activity."""
    items = []
    blocks = re.split(r'^\d+\.\s+', content, flags=re.MULTILINE)

    for block in blocks:
        if not block.strip():
            continue

        lines = block.split('\n')
        sentence = lines[0].strip() if lines else ''
        error_word = ''
        correct_form = ''
        options = []
        explanation = ''

        for line in lines[1:]:
            stripped = line.strip()
            if stripped.startswith('> [!error]'):
                error_word = stripped.replace('> [!error]', '').strip()
            elif stripped.startswith('> [!answer]') or stripped.startswith('> [!correct]'):
                correct_form = re.sub(r'^> \[!(answer|correct)\]\s*', '', stripped).strip()
            elif stripped.startswith('> [!options]'):
                opts_str = stripped.replace('> [!options]', '').strip()
                options = [o.strip() for o in opts_str.split('|') if o.strip()]
            elif stripped.startswith('> [!explanation]'):
                explanation = stripped.replace('> [!explanation]', '').strip()

        if sentence and error_word and correct_form:
            # Ensure correct_form is in options
            if options and correct_form not in options:
                options.append(correct_form)
            items.append(ErrorCorrectionItem(
                sentence=sentence,
                errorWord=error_word,
                correctForm=correct_form,
                options=options,
                explanation=explanation
            ))

    return items

def parse_cloze(content: str) -> ClozeData:
    """Parse cloze activity with passage and blanks.

    Format (new - inline options):
    - Passage with [___:N] markers for blanks
    - Numbered answer sections:
      N. opt1 | opt2 | opt3
         > [!answer] correct_answer

    Format (legacy - [!options] callout):
    - Passage with [___:N] markers for blanks
    - Numbered answer sections:
      N. default_answer
      > [!answer] correct_answer
      > [!options] opt1 | opt2 | opt3
    """
    lines = content.split('\n')
    passage_lines = []
    blanks = {}  # Use dict to maintain blank number order
    current_blank_num = None
    current_answer = None
    current_options = []

    # First pass: separate passage from answer blocks
    in_answers = False
    for line in lines:
        stripped = line.strip()

        # Check for numbered answer line (e.g., "1. –≤—ñ–¥ | –¥–ª—è | –ø—Ä–æ" or "1. –≤—ñ–¥")
        num_match = re.match(r'^(\d+)\.\s*(.*)$', stripped)
        if num_match:
            # Save previous blank if exists
            if current_blank_num is not None and current_answer:
                blanks[current_blank_num] = {"answer": current_answer, "options": current_options if current_options else [current_answer]}

            current_blank_num = int(num_match.group(1))
            default_val = num_match.group(2).strip()

            # Check if options are inline (new format: "1. opt1 | opt2 | opt3")
            if '|' in default_val:
                current_options = [o.strip() for o in default_val.split('|')]
                current_answer = current_options[0]  # First option as default answer
            else:
                current_answer = default_val  # Default answer from the line itself
                current_options = []

            in_answers = True
            continue

        # Parse answer callout (overrides default answer)
        if stripped.startswith('> [!answer]'):
            current_answer = stripped.replace('> [!answer]', '').strip()
            continue

        # Parse options callout (legacy format)
        if stripped.startswith('> [!options]'):
            opts = stripped.replace('> [!options]', '').strip()
            current_options = [o.strip() for o in opts.split('|')]
            continue

        # If we haven't started answers yet, it's passage
        if not in_answers:
            passage_lines.append(line)

    # Save last blank
    if current_blank_num is not None and current_answer:
        blanks[current_blank_num] = {"answer": current_answer, "options": current_options if current_options else [current_answer]}

    # Convert to ordered list
    blanks_list = []
    for i in sorted(blanks.keys()):
        blanks_list.append(blanks[i])

    return ClozeData(passage='\n'.join(passage_lines).strip(), blanks=blanks_list)

def parse_select(content: str) -> list[SelectQuestion]:
    """Parse select (multi-choice) activity."""
    questions = []
    blocks = re.split(r'^\d+\.\s+', content, flags=re.MULTILINE)

    for block in blocks:
        if not block.strip():
            continue

        lines = block.split('\n')
        question = ''
        options = []

        for line in lines:
            stripped = line.strip()
            if stripped.startswith('- [x]'):
                options.append({"text": stripped[5:].strip(), "correct": True})
            elif stripped.startswith('- [ ]'):
                options.append({"text": stripped[5:].strip(), "correct": False})
            elif not options and not stripped.startswith('-'):
                question = (question + ' ' + stripped).strip()

        if options:
            questions.append(SelectQuestion(question=question, options=options))

    return questions

def parse_translate(content: str) -> list[TranslateQuestion]:
    """Parse translate activity with checkbox or callout format."""
    questions = []
    blocks = re.split(r'^\d+\.\s+', content, flags=re.MULTILINE)

    for block in blocks:
        if not block.strip():
            continue

        lines = block.split('\n')
        source = ''
        options = []
        answer = ''
        option_texts = []

        for line in lines:
            stripped = line.strip()
            # Checkbox format
            if stripped.startswith('- [x]'):
                options.append({"text": stripped[5:].strip(), "correct": True})
            elif stripped.startswith('- [ ]'):
                options.append({"text": stripped[5:].strip(), "correct": False})
            # Callout format
            elif stripped.startswith('> [!answer]'):
                answer = stripped.replace('> [!answer]', '').strip()
            elif stripped.startswith('> [!options]'):
                option_texts = [o.strip() for o in stripped.replace('> [!options]', '').split('|')]
            elif not options and not option_texts and not stripped.startswith('-') and not stripped.startswith('>'):
                source = (source + ' ' + stripped).strip()

        # Convert callout format to options
        if answer and option_texts:
            for opt in option_texts:
                options.append({"text": opt, "correct": opt == answer})

        if options:
            questions.append(TranslateQuestion(source=source, options=options))

    return questions

# =============================================================================
# JSX GENERATORS
# =============================================================================

def parse_mark_the_words(content: str) -> list[MarkTheWordsItem]:
    """Parse mark-the-words content into items.

    Format: [word](correct) or [word](category) marks correct words
    Each paragraph (separated by ---) is a separate item
    """
    items = []
    # Split by horizontal rule to get separate exercises
    paragraphs = re.split(r'\n---+\n', content.strip())

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        # Find all marked words: [word](category)
        correct_words = []
        pattern = r'\[([^\]]+)\]\([^)]+\)'
        for match in re.finditer(pattern, para):
            word = match.group(1).strip()
            correct_words.append(word)

        # Remove the markdown-style marks to get plain text
        plain_text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', para)

        if correct_words:
            items.append(MarkTheWordsItem(text=plain_text, correctWords=correct_words))

    return items

def has_morpheme_patterns(content: str) -> bool:
    """Detect if content uses morpheme highlighting patterns.

    Supports pattern types:
    - *prefix*rest (e.g., *–ø—Ä–∏*–π—à–æ–≤)
    - rest*suffix* (e.g., –ß–∏—Ç*–∞—á*)

    DOES NOT match mark-the-words patterns:
    - space *word* space (e.g., " *–¥—Ä—É–≥–æ–≤—ñ* ")

    Key distinction: Morpheme patterns have Cyrillic letters touching at least one asterisk.
    """
    # Find all *...* patterns with context
    # Check if Cyrillic letters are adjacent to the asterisks
    # Morpheme: [–∞-—è]*X* or *X*[–∞-—è] (Cyrillic touching asterisk)
    # Mark-the-words: [^–∞-—è]*X*[^–∞-—è] (no Cyrillic touching asterisks)

    # Pattern: match *...* with one character before and after for context
    matches = list(re.finditer(r'(.?)\*([–∞-—è—ñ—ó—î“ë–ê-–Ø–Ü–á–Ñ“ê\s]+)\*(.?)', content, re.IGNORECASE))

    for match in matches:
        before = match.group(1)  # Character before opening *
        inside = match.group(2)  # Content between * *
        after = match.group(3)   # Character after closing *

        # Check if Cyrillic letter touches either asterisk
        before_is_cyrillic = before and re.match(r'[–∞-—è—ñ—ó—î“ë–ê-–Ø–Ü–á–Ñ“ê]', before, re.IGNORECASE)
        after_is_cyrillic = after and re.match(r'[–∞-—è—ñ—ó—î“ë–ê-–Ø–Ü–á–Ñ“ê]', after, re.IGNORECASE)

        # Morpheme pattern: at least one side has Cyrillic touching the asterisk
        if before_is_cyrillic or after_is_cyrillic:
            return True

    return False

def parse_highlight_morphemes(content: str) -> HighlightMorphemesItem:
    """Parse morpheme highlighting content.

    Supports pattern types:
    - *prefix*rest (e.g., *–ø—Ä–∏*–π—à–æ–≤ ‚Üí highlight "–ø—Ä–∏" in "–ø—Ä–∏–π—à–æ–≤")
    - rest*suffix* (e.g., –ß–∏—Ç*–∞—á* ‚Üí highlight "–∞—á" in "–ß–∏—Ç–∞—á")
    - *wholeWord* (e.g., *–ß–∏—Ç–∞—á* ‚Üí highlight entire word "–ß–∏—Ç–∞—á")
    - *multi-word phrase* (e.g., *–ú–µ–Ω—ñ –±—ñ–ª—å—à–µ –ø–æ–¥–æ–±–∞—î—Ç—å—Å—è* ‚Üí highlight entire phrase)

    If the first line has no morphemes, treat it as an instruction.
    """
    morphemes = []
    instruction = ''

    # Check if first line is an instruction (no morphemes)
    lines = content.strip().split('\n')
    first_line = lines[0] if lines else ''

    # Pattern: (prefix)*morpheme*(suffix)
    # Group 1: optional text before * (prefix of word)
    # Group 2: morpheme inside * * (can be single word or multi-word phrase)
    # Group 3: optional text after * (suffix of word)
    pattern = r'([–∞-—è—ñ—ó—î“ë–ê-–Ø–Ü–á–Ñ“ê]*)\*([–∞-—è—ñ—ó—î“ë–ê-–Ø–Ü–á–Ñ“ê\s]+)\*([–∞-—è—ñ—ó—î“ë–ê-–Ø–Ü–á–Ñ“ê]*)'

    # If first line has no morphemes, it's likely an instruction
    if first_line and not re.search(pattern, first_line):
        instruction = first_line.strip()
        # Remove instruction from content
        content = '\n'.join(lines[1:]).strip()

    for match in re.finditer(pattern, content):
        prefix = match.group(1)      # Text before morpheme
        morpheme = match.group(2)    # The morpheme to highlight
        suffix = match.group(3)      # Text after morpheme

        # Construct the full word
        full_word = prefix + morpheme + suffix

        # Detect morpheme type based on position
        if not prefix and suffix:
            morph_type = 'prefix'
        elif prefix and not suffix:
            morph_type = 'suffix'
        elif not prefix and not suffix:
            morph_type = 'whole'
        else:
            morph_type = 'root'

        morphemes.append(MorphemeItem(
            word=full_word,
            morpheme=morpheme,
            type=morph_type
        ))

    # Remove asterisks to get plain text
    plain_text = re.sub(r'[–∞-—è—ñ—ó—î“ë–ê-–Ø–Ü–á–Ñ“ê]*\*([–∞-—è—ñ—ó—î“ë–ê-–Ø–Ü–á–Ñ“ê\s]+)\*[–∞-—è—ñ—ó—î“ë–ê-–Ø–Ü–á–Ñ“ê]*',
                        lambda m: m.group(0).replace('*', ''), content)

    return HighlightMorphemesItem(text=plain_text.strip(), morphemes=morphemes, instruction=instruction)

def parse_essay_response(content: str) -> EssayResponseData:
    """Parse essay-response activity."""
    prompt_lines = []
    model_answer_lines = []
    rubric_lines = []
    
    current_section = 'prompt'
    
    lines = content.split('\n')
    for line in lines:
        if line.strip().startswith('> [!model-answer]'):
            current_section = 'model'
            # Skip the callout header itself? No, we might want to keep the content inside
            # But the callout syntax is handled by convert_callouts? 
            # No, if we wrap it in a component, we want the RAW content inside the callout
            # to pass as a prop.
            continue
        elif line.strip().startswith('> [!rubric]'):
            current_section = 'rubric'
            continue
            
        if current_section == 'prompt':
            prompt_lines.append(line)
        elif current_section == 'model':
            # Remove '>' prefix from callout content if present
            clean_line = re.sub(r'^>\s?', '', line)
            model_answer_lines.append(clean_line)
        elif current_section == 'rubric':
            clean_line = re.sub(r'^>\s?', '', line)
            rubric_lines.append(clean_line)
            
    return EssayResponseData(
        prompt='\n'.join(prompt_lines).strip(),
        modelAnswer='\n'.join(model_answer_lines).strip(),
        rubric='\n'.join(rubric_lines).strip()
    )

def parse_comparative_study(content: str) -> ComparativeStudyData:
    """Parse comparative-study activity."""
    # Split by Task/Assignment header if possible, or just extract model answer
    # Structure: 
    # Content (Tables/Text)
    # **Task:** ...
    # > [!model-answer]
    
    content_lines = []
    task_lines = []
    model_lines = []
    
    current_section = 'content'
    
    lines = content.split('\n')
    for line in lines:
        stripped = line.strip()
        
        if stripped.startswith('> [!model-answer]'):
            current_section = 'model'
            continue
        
        # Detect task header (bold "Task:" or "–ó–∞–≤–¥–∞–Ω–Ω—è:")
        if current_section == 'content' and re.match(r'\*\*(Task|–ó–∞–≤–¥–∞–Ω–Ω—è).*?:?\*\*', stripped, re.IGNORECASE):
            current_section = 'task'
            task_lines.append(line) # Keep the header line as part of task
            continue
            
        if current_section == 'content':
            content_lines.append(line)
        elif current_section == 'task':
            task_lines.append(line)
        elif current_section == 'model':
            clean_line = re.sub(r'^>\s?', '', line)
            model_lines.append(clean_line)
            
    return ComparativeStudyData(
        content='\n'.join(content_lines).strip(),
        task='\n'.join(task_lines).strip(),
        modelAnswer='\n'.join(model_lines).strip()
    )

def highlight_morphemes_to_jsx(item: HighlightMorphemesItem, title: str, is_ukrainian_forced: bool = False) -> str:
    """Convert morpheme highlighting item to JSX HighlightMorphemesActivity component."""
    if not item.morphemes:
        return ''

    # Build morphemes array for JSX
    morphemes_jsx_parts = []
    for m in item.morphemes:
        morphemes_jsx_parts.append(
            f'{{ word: `{escape_jsx(m.word)}`, morpheme: `{escape_jsx(m.morpheme)}`, type: `{m.type}` }}'
        )

    morphemes_jsx = ',\n    '.join(morphemes_jsx_parts)

    instruction_jsx = ''
    if item.instruction:
        instruction_jsx = f'\n  instruction={{`{escape_jsx(item.instruction)}`}}'

    return f'''### {title}

<HighlightMorphemes isUkrainian={{{'true' if is_ukrainian_forced else 'false'}}}>
  <HighlightMorphemesActivity{instruction_jsx}
    text={{`{escape_jsx(item.text)}`}}
    morphemes={{[
    {morphemes_jsx}
  ]}}
  />
</HighlightMorphemes>'''

def essay_response_to_jsx(data: EssayResponseData, title: str, is_ukrainian_forced: bool = False) -> str:
    """Convert essay-response data to JSX EssayResponse component."""
    return f'''### {title}

<EssayResponse
  title="{escape_jsx(title)}"
  prompt={{`{escape_jsx(data.prompt)}`}}
  modelAnswer={{`{escape_jsx(data.modelAnswer)}`}}
  rubric={{`{escape_jsx(data.rubric)}`}}
  isUkrainian={{{'true' if is_ukrainian_forced else 'false'}}}
/>'''

def comparative_study_to_jsx(data: ComparativeStudyData, title: str, is_ukrainian_forced: bool = False) -> str:
    """Convert comparative-study data to JSX ComparativeStudy component."""
    return f'''### {title}

<ComparativeStudy
  title="{escape_jsx(title)}"
  content={{`{escape_jsx(data.content)}`}}
  task={{`{escape_jsx(data.task)}`}}
  modelAnswer={{`{escape_jsx(data.modelAnswer)}`}}
  isUkrainian={{{'true' if is_ukrainian_forced else 'false'}}}
/>'''

# =============================================================================
# CONTENT PROCESSING
# =============================================================================

# Callout mapping
CALLOUT_MAP = {
    'tip': {'type': 'tip'},
    'note': {'type': 'note'},
    'warning': {'type': 'warning'},
    'important': {'type': 'warning'},
    'caution': {'type': 'caution'},
    'info': {'type': 'info'},
    'observe': {'type': 'tip', 'icon': 'üîç', 'title': 'Pattern Discovery'},
    'resources': {'type': 'info', 'icon': 'üéß', 'title': 'External Resources'},
    'example': {'type': 'info', 'icon': 'üìù', 'title': 'Example'},
    'conversation': {'type': 'note', 'icon': 'üí¨', 'title': 'Conversation'},
    'summary': {'type': 'note', 'icon': 'üìã', 'title': 'Summary'},
    'solution': {'type': 'solution'},  # Collapsible answer reveal for checkpoints
    'model-answer': {'type': 'success', 'icon': '‚úÖ', 'title': 'Model Answer'},
    'rubric': {'type': 'info', 'icon': 'üìä', 'title': 'Rubric'},
    'analysis': {'type': 'info', 'icon': 'üßê', 'title': 'Analysis'},
    'history-bite': {'type': 'info', 'icon': 'üï∞Ô∏è', 'title': 'History Bite'},
    'myth-buster': {'type': 'danger', 'icon': 'üõ°Ô∏è', 'title': 'Myth Buster'},
    'quote': {'type': 'note', 'icon': 'üìú', 'title': 'Quote'},
    'context': {'type': 'info', 'icon': 'üåç', 'title': 'Context'},
    'legacy': {'type': 'tip', 'icon': 'üíé', 'title': 'Legacy'},
    'reflection': {'type': 'info', 'icon': 'ü§î', 'title': 'Reflection'},
    'source': {'type': 'note', 'icon': 'üìñ', 'title': 'Source'},
}

def convert_callouts(content: str) -> str:
    """Convert GitHub-style callouts to Docusaurus admonitions."""
    lines = content.split('\n')
    result = []
    i = 0

    while i < len(lines):
        line = lines[i]

        # Check for callout start: > [!type] (may have leading whitespace)
        # Allow hyphens in type (e.g. model-answer)
        callout_match = re.match(r'^(\s*)>\s*\[!([\w-]+)\]\s*(.*)', line)
        if callout_match:
            indent = callout_match.group(1)  # Preserve indentation for output
            callout_type = callout_match.group(2).lower()
            title_extra = callout_match.group(3).strip()

            config = CALLOUT_MAP.get(callout_type, {'type': 'note'})
            admon_type = config['type']

            # Build title
            if title_extra:
                title = title_extra
            elif 'title' in config:
                icon = config.get('icon', '')
                title = f"{icon} {config['title']}" if icon else config['title']
            else:
                title = callout_type.title()

            # Collect callout content - check for continuation lines (indented blockquotes too)
            callout_lines = []
            i += 1
            while i < len(lines):
                # Match continuation: optional whitespace + > + content
                cont_match = re.match(r'^\s*>(.*)', lines[i])
                if cont_match:
                    callout_lines.append(cont_match.group(1).strip() if cont_match.group(1) else '')
                    i += 1
                else:
                    break

            # Special handling for solution callouts - use HTML details for collapsible
            if callout_type == 'solution':
                result.append(f'<details className="solution-block">')
                result.append(f'<summary>{title}</summary>')
                result.append('')
                result.extend(callout_lines)
                result.append('')
                result.append('</details>')
                result.append('')
            else:
                # Output Docusaurus admonition
                result.append(f':::{admon_type}[{title}]')
                result.extend(callout_lines)
                result.append(':::')
                result.append('')
        else:
            result.append(line)
            i += 1

    return '\n'.join(result)

def process_story_sections(content: str) -> str:
    """Add blank lines between narrative lines in story sections.

    Story sections (### Story Time, ### Dialogue, etc.) contain narrative
    paragraphs that need blank lines between them for proper Markdown rendering.
    This function detects story headers and adds blank lines between non-blank
    lines until the next header.

    Note: Dialog lines (starting with ‚Äî) are left consecutive so that
    process_dialogues can wrap them in conversation containers.
    """
    lines = content.split('\n')
    result = []
    i = 0

    # Pattern to detect story section headers
    story_header_pattern = re.compile(r'^###\s+(Story|Dialogue|Reading|Conversation|Text|Passage)', re.IGNORECASE)
    # Pattern to detect any header (to know when story section ends)
    any_header_pattern = re.compile(r'^#{1,4}\s+')

    while i < len(lines):
        line = lines[i]
        stripped = line.strip()

        # Check if this is a story section header
        if story_header_pattern.match(stripped):
            result.append(line)
            result.append('')
            i += 1

            # Process lines until next header or end of content
            while i < len(lines):
                current_line = lines[i]
                current_stripped = current_line.strip()

                # Stop at next header
                if any_header_pattern.match(current_stripped):
                    break

                # Skip already-blank lines
                if not current_stripped:
                    result.append(current_line)
                    i += 1
                    continue

                # Add the line
                result.append(current_line)

                # Look ahead - if next line is non-blank content (not a header, not blank),
                # add a blank line after current line UNLESS both lines are dialog lines (‚Äî)
                if i + 1 < len(lines):
                    next_stripped = lines[i + 1].strip()
                    current_is_dialog = current_stripped.startswith('‚Äî')
                    next_is_dialog = next_stripped.startswith('‚Äî')

                    # Add blank line only if NOT both dialog lines
                    if next_stripped and not any_header_pattern.match(next_stripped):
                        if not (current_is_dialog and next_is_dialog):
                            result.append('')

                i += 1
        else:
            result.append(line)
            i += 1

    return '\n'.join(result)


def process_dialogues(content: str) -> str:
    """Group consecutive dialog lines into conversation blocks.

    Detects sequences of lines starting with em-dash (‚Äî) and wraps
    them in a styled conversation container. Separates Ukrainian
    from English translations (they're separated by blank lines).
    """
    lines = content.split('\n')
    result = []
    i = 0

    # Track if we're inside a JSX component
    inside_jsx = 0

    while i < len(lines):
        line = lines[i]
        stripped = line.strip()

        # Track JSX component depth
        if re.match(r'^<[A-Z][a-zA-Z]*', stripped):
            inside_jsx += 1
        if stripped.endswith('/>'):
            inside_jsx = max(0, inside_jsx - 1)
        if re.match(r'^</[A-Z]', stripped):
            inside_jsx = max(0, inside_jsx - 1)

        # Check if this starts a dialog sequence (outside JSX)
        if stripped.startswith('‚Äî') and inside_jsx == 0 and '`' not in line:
            # Collect consecutive dialog lines (stop at blank line)
            dialog_lines = []
            while i < len(lines):
                current = lines[i].strip()
                if current.startswith('‚Äî') and '`' not in lines[i]:
                    dialog_lines.append(lines[i])
                    i += 1
                else:
                    # Stop at any non-dialog line (including blank)
                    break

            # Only wrap if we have 2+ dialog lines (actual conversation)
            if len(dialog_lines) >= 2:
                result.append('<div className="conversation">')
                result.append('')
                for dl in dialog_lines:
                    result.append(dl)
                    result.append('')
                result.append('</div>')
            else:
                # Single line - just keep as-is
                for dl in dialog_lines:
                    result.append(dl)
        else:
            result.append(line)
            i += 1

    return '\n'.join(result)


# =============================================================================
# SLUG LINK RESOLVER
# =============================================================================

def resolve_slug_links(content: str) -> str:
    """
    Replace [slug:xxx] links with actual module title and path.

    This allows content to reference other modules by slug instead of
    hardcoded paths, making the curriculum resilient to renumbering.

    Example:
        Input:  See [slug:the-cyrillic-code-i] for details.
        Output: See [The Cyrillic Code I](/a1/module-01) for details.

    Args:
        content: Markdown content with potential [slug:xxx] links

    Returns:
        Content with slug links resolved to actual paths
    """
    def replace_slug(match):
        slug = match.group(1)
        try:
            module = get_module_by_slug(slug)
            if module:
                return f"[{module.title}]({module.path})"
            else:
                # Module not found - leave as-is but log warning
                print(f"  Warning: Unknown slug '{slug}' - link not resolved")
                return match.group(0)
        except Exception as e:
            print(f"  Warning: Error resolving slug '{slug}': {e}")
            return match.group(0)

    # Match [slug:xxx] pattern
    return re.sub(r'\[slug:([a-z0-9-]+)\]', replace_slug, content)


# =============================================================================
# MDX GENERATOR
# =============================================================================

def generate_mdx(md_content: str, module_num: int, yaml_activities: list[Activity] | None = None, meta_data: dict | None = None, vocab_items: list[dict] | None = None, external_resources: dict | None = None, level: str = 'a1') -> str:
    """Convert markdown content to MDX.

    Args:
        md_content: Markdown content
        module_num: Module number for sidebar
        yaml_activities: Optional list of activities from ActivityParser (takes precedence over embedded)
        meta_data: Optional metadata from YAML (replaces frontmatter)
        vocab_items: Optional vocab list from YAML
        external_resources: Optional external resources dict (injected from YAML)
        level: Current level (used for specialized formatting like LIT)
    """
    if meta_data:
        fm = meta_data
        body = md_content # MD file is already stripped if meta exists (usually)
        # But if we are transitioning, MD might still have FM. 
        # parse_frontmatter splits it regardless.
        _, body = parse_frontmatter(md_content)
    else:
        fm, body = parse_frontmatter(md_content)

    # Determine if Ukrainian headers are forced
    is_ukrainian_forced = False
    lvl = level.lower()
    if lvl in ['b2', 'c1', 'c2', 'lit']:
        is_ukrainian_forced = True
    elif lvl == 'b1' and module_num > 5:
        is_ukrainian_forced = True

    # Component imports
    imports = """import Quiz from '@site/src/components/Quiz';
import MatchUp from '@site/src/components/MatchUp';
import FillIn from '@site/src/components/FillIn';
import TrueFalse from '@site/src/components/TrueFalse';
import Unjumble from '@site/src/components/Unjumble';
import GroupSort from '@site/src/components/GroupSort';
import Anagram from '@site/src/components/Anagram';
import ErrorCorrection, { ErrorCorrectionItem } from '@site/src/components/ErrorCorrection';
import Cloze from '@site/src/components/Cloze';
import Select from '@site/src/components/Select';
import Translate from '@site/src/components/Translate';
import MarkTheWords, { MarkTheWordsActivity } from '@site/src/components/MarkTheWords';
import HighlightMorphemes, { HighlightMorphemesActivity } from '@site/src/components/HighlightMorphemes';
import EssayResponse from '@site/src/components/EssayResponse';
import ComparativeStudy from '@site/src/components/ComparativeStudy';
import ReadingActivity from '@site/src/components/ReadingActivity';
import CriticalAnalysis from '@site/src/components/CriticalAnalysis';
import AuthorialIntent from '@site/src/components/AuthorialIntent';"""

    # Frontmatter
    frontmatter = f'''---
sidebar_position: {module_num}
sidebar_label: "{str(module_num).zfill(2)}. {escape_jsx(fm.get('title', 'Untitled'))}"
title: "{escape_jsx(fm.get('title', 'Untitled'))}"
description: "{escape_jsx(fm.get('subtitle', ''))}"
---
'''

    # 1. Clean up body: Remove existing Vocabulary, Activities, and Resources placeholders
    # Remove Activities
    body = re.sub(r'(^#{1,2}\s+(?:Activities|–í–ø—Ä–∞–≤–∏))[\s\S]*?(?=\n#{1,2}|\Z)', '', body, flags=re.MULTILINE)
    # Remove Vocabulary
    body = re.sub(r'(^#{1,2}\s+(?:Vocabulary|–°–ª–æ–≤–Ω–∏–∫))[\s\S]*?(?=\n#{1,2}|\Z)', '', body, flags=re.MULTILINE)
    # Remove Resources callout
    body = re.sub(r'>\s*\[!resources\].*?(\n>.*)*', '', body, flags=re.MULTILINE | re.IGNORECASE)

    # 2. Append new Resources (appears after Summary)
    if external_resources:
        resources_md = format_resources_for_mdx(external_resources, is_ukrainian_forced)
        if resources_md:
            body = body.rstrip() + '\n\n' + resources_md

    # 3. Append new Vocabulary (appears last)
    if vocab_items:
        vocab_header = "–°–ª–æ–≤–Ω–∏–∫" if is_ukrainian_forced else "Vocabulary"
        vocab_md = _vocab_items_to_markdown(vocab_items, vocab_header) if level.lower() != 'lit' else _lit_vocab_items_to_markdown(vocab_items, vocab_header)
        body = body.rstrip() + '\n\n' + vocab_md

    # 4. Process activities (Injected BEFORE Vocabulary)
    if yaml_activities:
        act_header = "–í–ø—Ä–∞–≤–∏" if is_ukrainian_forced else "Activities"
        activities_jsx = f'## üéØ {act_header}\n\n' + yaml_activities_to_jsx(yaml_activities, is_ukrainian_forced)
        
        # Injected before Vocabulary/–°–ª–æ–≤–Ω–∏–∫
        inject_match = re.search(r'\n(#{1,2}\s+(?:Vocabulary|–°–ª–æ–≤–Ω–∏–∫))', body)
        if inject_match:
            processed = body[:inject_match.start()] + '\n\n' + activities_jsx + '\n' + body[inject_match.start():]
        else:
            processed = body + '\n\n' + activities_jsx
    else:
        processed = body

    # Convert callouts
    processed = convert_callouts(processed)

    # Resolve slug links (e.g., [slug:the-cyrillic-code-i] -> [The Cyrillic Code I](/a1/module-01))
    processed = resolve_slug_links(processed)

    # Fix HTML for JSX compatibility (self-closing tags)
    processed = fix_html_for_jsx(processed)

    # Process story sections (add blank lines between narrative lines)
    processed = process_story_sections(processed)

    # Process dialogues (wrap em-dash lines in conversation divs)
    processed = process_dialogues(processed)

    # Remove duplicate H1 title
    processed = re.sub(r'^#\s+[^\n]+\n', '', processed, count=1)

    # Add emojis to H2 section headings for TOC (Standardize to H2 for Docusaurus)
    
    header_map = {
        'Summary': '–ü—ñ–¥—Å—É–º–æ–∫',
        'Vocabulary': '–°–ª–æ–≤–Ω–∏–∫',
        'Self-Assessment': '–°–∞–º–æ–æ—Ü—ñ–Ω–∫–∞',
        'External Resources': '–ó–æ–≤–Ω—ñ—à–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏'
    }

    # Summary
    sum_text = header_map['Summary'] if is_ukrainian_forced else r'\g<1>'
    processed = re.sub(r'^#{1,2} (Summary|–ü—ñ–¥—Å—É–º–æ–∫)', f'## üìã {sum_text}', processed, flags=re.MULTILINE)
    
    # Vocabulary
    vocab_text = header_map['Vocabulary'] if is_ukrainian_forced else r'\g<1>'
    processed = re.sub(r'^#{1,2} (Vocabulary|–°–ª–æ–≤–Ω–∏–∫)', f'## üìö {vocab_text}', processed, flags=re.MULTILINE)
    
    # Self-Assessment
    sa_text = header_map['Self-Assessment'] if is_ukrainian_forced else r'\g<1>'
    processed = re.sub(r'^#{1,2} (Self-Assessment|–°–∞–º–æ–æ—Ü—ñ–Ω–∫–∞)', f'## ‚úÖ {sa_text}', processed, flags=re.MULTILINE)
    
    # External Resources (Note: Resources from YAML are injected as callouts, but if converted to headers later, this catches them)
    # Actually, resources from YAML use > [!resources] which converts to :::info. 
    # But if there are manual H2 resources sections, this standardizes them.
    ext_text = header_map['External Resources'] if is_ukrainian_forced else r'\g<1>'
    processed = re.sub(r'^#{1,2} (External Resources?|–ó–æ–≤–Ω—ñ—à–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏|Resources|–†–µ—Å—É—Ä—Å–∏)', f'## üîó {ext_text}', processed, flags=re.MULTILINE)

    # Build MDX
    parts = [frontmatter, imports, '', processed]

    return '\n'.join(parts)

# =============================================================================
# EXTERNAL RESOURCES
# =============================================================================

def validate_and_clean_url(url: str, title: str = '') -> str:
    """
    Validate and clean URL for markdown link formatting.

    Detects and fixes common URL issues:
    - Incomplete angle brackets: <https://...  ‚Üí https://...
    - Unmatched parentheses in URL

    Args:
        url: The URL string to validate
        title: Optional title for error messages

    Returns:
        Cleaned URL string

    Raises:
        Warning if URL has issues
    """
    if not url:
        return url

    original_url = url

    # Remove angle brackets if present (not needed in YAML, causes issues)
    if url.startswith('<'):
        if not url.endswith('>'):
            print(f"  ‚ö†Ô∏è  Malformed URL (missing closing '>'): {title}")
            print(f"      {url}")
            url = url.lstrip('<')
        else:
            url = url[1:-1]  # Remove both angle brackets

    # Check for unmatched parentheses
    open_parens = url.count('(')
    close_parens = url.count(')')
    if open_parens != close_parens:
        print(f"  ‚ö†Ô∏è  URL has unmatched parentheses: {title}")
        print(f"      {url}")
        print(f"      Expected {open_parens} closing parentheses, found {close_parens}")

    if url != original_url:
        print(f"      Fixed to: {url}")

    return url

def format_resources_for_mdx(resources: dict, is_ukrainian_forced: bool = False) -> str:
    """
    Format external resources for MDX output (emoji template).

    Args:
        resources: Dict with keys: podcasts, youtube, articles, books, websites
        is_ukrainian_forced: Whether to force Ukrainian headers

    Returns:
        Formatted markdown string for [!resources] callout block
    """
    if not resources or not any(resources.get(t) for t in ['podcasts', 'youtube', 'articles', 'books', 'websites']):
        return ""

    header_title = "–ó–æ–≤–Ω—ñ—à–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏" if is_ukrainian_forced else "External Resources"
    
    lines = []
    lines.append(f"> [!resources] üîó {header_title}")
    lines.append(">")

    # Emoji icons per resource type
    display_names = {
        'Podcasts': '–ü–æ–¥–∫–∞—Å—Ç–∏',
        'YouTube': 'YouTube',
        'Articles': '–°—Ç–∞—Ç—Ç—ñ',
        'Books': '–ö–Ω–∏–≥–∏',
        'Websites': '–°–∞–π—Ç–∏'
    }

    resource_config = [
        ('podcasts', 'üéß', 'Podcasts'),
        ('youtube', 'üì∫', 'YouTube'),
        ('articles', 'üìñ', 'Articles'),
        ('books', 'üìö', 'Books'),
        ('websites', 'üåê', 'Websites')
    ]

    # Priority and relevance maps for sorting
    # Priority 1 = highest (Ukrainian Lessons Priority 1), 5 = lowest
    priority_map = {1: 5, 2: 4, 3: 3, 4: 2, 5: 1, None: 0}
    relevance_priority = {'high': 3, 'medium': 2, 'low': 1}

    for resource_type, icon, display_name in resource_config:
        items = resources.get(resource_type, [])
        if not items:
            continue
            
        final_display_name = display_names.get(display_name, display_name) if is_ukrainian_forced else display_name

        # Sort by: priority (1‚Üí5, highest first) ‚Üí relevance (high‚Üílow) ‚Üí title (A‚ÜíZ)
        sorted_items = sorted(
            items,
            key=lambda x: (
                -priority_map.get(x.get('priority'), 0),  # Priority 1 appears first
                -relevance_priority.get(x.get('relevance', 'low'), 0),  # High relevance next
                x.get('title', '').lower()  # Alphabetical last
            )
        )

        # Add section header
        lines.append(f"> **{icon} {final_display_name}:**")

        # Format each item
        for item in sorted_items:
            title = item.get('title', 'Unknown')
            url = validate_and_clean_url(item.get('url', ''), title)

            if resource_type == 'podcasts':
                desc = item.get('match_reason') or item.get('description', '')
                if desc:
                    lines.append(f"> - [{title}]({url}) ‚Äî {desc}")
                else:
                    lines.append(f"> - [{title}]({url})")

            elif resource_type == 'youtube':
                channel = item.get('channel', '')
                desc = item.get('description', channel)
                if desc:
                    lines.append(f"> - [{title}]({url}) ‚Äî {desc}")
                else:
                    lines.append(f"> - [{title}]({url})")

            elif resource_type == 'articles':
                source = item.get('source', '')
                desc = item.get('description', source)
                if desc:
                    lines.append(f"> - [{title}]({url}) ‚Äî {desc}")
                else:
                    lines.append(f"> - [{title}]({url})")

            elif resource_type == 'books':
                author = item.get('author', 'Unknown')
                pages = item.get('pages', '')
                desc = item.get('description', '')

                parts = [f"{title} by {author}"]
                if pages:
                    parts.append(f"(pages: {pages})")
                if desc:
                    parts.append(f"‚Äî {desc}")
                lines.append(f"> - {' '.join(parts)}")

            elif resource_type == 'websites':
                source = item.get('source', '')
                desc = item.get('description', source)
                if desc:
                    lines.append(f"> - [{title}]({url}) ‚Äî {desc}")
                else:
                    lines.append(f"> - [{title}]({url})")

        # Add blank line between sections
        lines.append(">")

    # Remove trailing blank line
    if lines and lines[-1] == ">":
        lines.pop()

    return '\n'.join(lines)

# =============================================================================
# MAIN
# =============================================================================

def _vocab_items_to_markdown(items: list[dict], header_text: str = "Vocabulary") -> str:
    lines = [
        f"## {header_text}",
        "",
        "| Word | IPA | English | POS | Gender | Note |",
        "| --- | --- | --- | --- | --- | --- |"
    ]
    for item in items:
        # Map gender m/f/n -> —á/–∂/—Å
        g_map = {'m': '—á', 'f': '–∂', 'n': '—Å', 'pl': 'pl', '-': '-', '': ''}
        raw_g = item.get('gender', '')
        g_val = g_map.get(raw_g, raw_g)
        
        # Map POS propn -> name
        raw_p = item.get('pos', '')
        p_val = 'name' if raw_p == 'propn' else raw_p
        
        line = f"| {item.get('lemma')} | {item.get('ipa','')} | {item.get('translation','')} | {p_val} | {g_val} | {item.get('usage','')} |"
        lines.append(line)
        
    return '\n'.join(lines)

from manifest_utils import load_manifest, get_module_by_slug, get_modules_for_level, Module

def get_modules_from_manifest(target_level: Optional[str] = None) -> list[Module]:
    """Get list of modules to process from manifest."""
    manifest = load_manifest()
    all_modules = []
    
    # Process core levels
    for level in ['a1', 'a2', 'b1', 'b2', 'c1', 'c2']:
        if target_level and level != target_level:
            continue
        all_modules.extend(get_modules_for_level(level))
        
    # Process tracks
    for track_name in manifest.get('tracks', {}):
        if track_name.startswith('_'): continue
        if target_level and track_name != target_level:
            continue
            
        track_data = manifest['tracks'][track_name]
        for local_num, mod_entry in enumerate(track_data.get('modules', []), 1):
            all_modules.append(Module(
                slug=mod_entry['slug'],
                title=mod_entry.get('title', 'Untitled'),
                level=track_name,
                track=track_name,
                local_num=local_num,
                global_num=0,
                phase=mod_entry.get('phase'),
                focus=mod_entry.get('focus')
            ))
            
    return all_modules

def main():
    args = sys.argv[1:]

    # Parse --validate flag
    validate_after = '--validate' in args
    args = [a for a in args if a != '--validate']

    print('\nüöÄ MDX Generator (Manifest-Driven)\n', flush=True)

    target_level = None
    target_module = None
    lang_pair = 'l2-uk-en'

    if args:
        if args[0].endswith('.md'):
            # Specific file logic (keeping for backward compatibility)
            file_path = Path(args[0])
            # ... existing detection logic could be here, but let's simplify for now
            # and rely on slug lookup if we have a filename
            slug = file_path.stem
            # Remove leading numbers if present
            slug = re.sub(r'^\d+-', '', slug)
            mod_obj = get_module_by_slug(slug)
            if mod_obj:
                process_modules = [mod_obj]
            else:
                print(f"  ‚ö†Ô∏è  Could not find module in manifest for: {args[0]}")
                sys.exit(1)
        else:
            lang_pair = args[0]
            target_level = args[1].lower() if len(args) > 1 else None
            target_module = int(args[2]) if len(args) > 2 else None
            process_modules = get_modules_from_manifest(target_level)
    else:
        process_modules = get_modules_from_manifest()

    print(f'Source: curriculum/{lang_pair}/', flush=True)
    print(f'Output: docusaurus/docs/\n', flush=True)

    # Load EXTERNAL RESOURCES
    external_resources_file = PROJECT_ROOT / 'docs' / 'resources' / 'external_resources.yaml'
    all_resources = {}
    if external_resources_file.exists():
        with open(external_resources_file, 'r', encoding='utf-8') as f:
            resources_data = yaml.safe_load(f)
            all_resources = resources_data.get('resources', {})
        print(f'üìö Loaded {len(all_resources)} modules with external resources\n', flush=True)

    current_level = None
    for mod in process_modules:
        if target_module and mod.local_num != target_module:
            continue

        if mod.level != current_level:
            print(f'üìÅ Level {mod.level.upper()}')
            current_level = mod.level
            output_dir = DOCUSAURUS_DIR / mod.level
            output_dir.mkdir(parents=True, exist_ok=True)

        # Find the physical file
        # Try slug-only first, then numbered slug (migration period)
        level_dir = CURRICULUM_DIR / lang_pair / mod.level
        md_file = level_dir / f"{mod.slug}.md"
        if not md_file.exists():
            md_file = level_dir / f"{mod.local_num:02d}-{mod.slug}.md"
        
        if not md_file.exists():
            print(f"  ‚ö†Ô∏è  Physical file not found for slug '{mod.slug}' in {mod.level}")
            continue

        # Read and convert
        md_content = md_file.read_text(encoding='utf-8')
        
        # Load META
        meta_file = level_dir / 'meta' / f"{mod.slug}.yaml"
        if not meta_file.exists():
            meta_file = level_dir / 'meta' / f"{mod.local_num:02d}-{mod.slug}.yaml"
            
        meta_data = None
        if meta_file.exists():
            with open(meta_file, 'r', encoding='utf-8') as f:
                meta_data = yaml.safe_load(f)
                
        # Load VOCABULARY
        vocab_file = level_dir / 'vocabulary' / f"{mod.slug}.yaml"
        if not vocab_file.exists():
            vocab_file = level_dir / 'vocabulary' / f"{mod.local_num:02d}-{mod.slug}.yaml"
            
        vocab_items = None
        if vocab_file.exists():
            with open(vocab_file, 'r', encoding='utf-8') as f:
                v_data = yaml.safe_load(f)
                if v_data and 'items' in v_data:
                    vocab_items = v_data['items']

        # Load ACTIVITIES
        yaml_file = level_dir / 'activities' / f"{mod.slug}.yaml"
        if not yaml_file.exists():
            yaml_file = level_dir / 'activities' / f"{mod.local_num:02d}-{mod.slug}.yaml"
        if not yaml_file.exists():
            yaml_file = level_dir / f"{mod.local_num:02d}-{mod.slug}.activities.yaml"
        
        yaml_activities = None
        if yaml_file.exists():
            parser = ActivityParser()
            try:
                yaml_activities = parser.parse(yaml_file)
            except Exception as e:
                print(f'    ‚ö†Ô∏è Error parsing YAML activities for {mod.slug}: {e}')

        # EXTERNAL RESOURCES
        module_id = f"{mod.level}-{mod.slug}"
        module_resources = all_resources.get(module_id, {})

        mdx_content = generate_mdx(md_content, mod.local_num, yaml_activities, meta_data, vocab_items, module_resources, mod.level)

        # Write output
        # Use slug-based filenames for tracks (b2-hist, c1-bio, etc.)
        # Use numbered format for core levels (a1-c2) for backward compatibility
        if mod.track and mod.track != 'core':
            output_file = output_dir / f'{mod.slug}.mdx'
        else:
            output_file = output_dir / f'module-{mod.local_num:02d}.mdx'
        output_file.write_text(mdx_content, encoding='utf-8')

        print(f'  ‚úì {mod.local_num:02d}. {mod.title}')

    print('\n‚úÖ MDX generation complete!')

    # Run validation if --validate flag was set
    if validate_after:
        print('\n' + '=' * 50)
        print('Running MDX validation...\n')
        import subprocess
        validate_args = [sys.executable, str(SCRIPT_DIR / 'validate_mdx.py'), lang_pair]
        if target_level:
            validate_args.append(target_level)
        if target_module:
            validate_args.append(str(target_module))
        result = subprocess.run(validate_args)
        sys.exit(result.returncode)

# =============================================================================
# VOCABULARY HELPERS
# =============================================================================

def _lit_vocab_items_to_markdown(items: list[dict], header_text: str = "Vocabulary") -> str:
    lines = [
        f"## {header_text}",
        "",
        "| –¢–µ—Ä–º—ñ–Ω/–°–ª–æ–≤–æ | –ü–µ—Ä–µ–∫–ª–∞–¥ | –ü—Ä–∏–º—ñ—Ç–∫–∏ |",
        "| --- | --- | --- |"
    ]
    for item in items:
        # Use lemma, translation, notes (standardized fields)
        lemma = item.get('lemma') or item.get('term', '')
        translation = item.get('translation') or item.get('definition', '')
        notes = item.get('notes') or item.get('comment', '')
        
        line = f"| **{lemma}** | {translation} | {notes} |"
        lines.append(line)
        
    return '\n'.join(lines)

if __name__ == '__main__':
    main()
