# Stage 4: Review & Fix Loop

> **‚ö†Ô∏è READ FIRST: `claude_extensions/NON-NEGOTIABLE-RULES.md`**
>
> All audit gates MUST pass. NO exceptions. NO negotiation. Work until ‚úÖ on ALL gates.

> **‚ö†Ô∏è CRITICAL: Always use `.venv/bin/python` for ALL Python scripts.**
> Never use `python3` or `python` directly - dependencies are in the venv.

Review the module, fix violations, repeat until PASS.

## SUCCESS CRITERIA

**A module is COMPLETE when:**
- ‚úÖ ALL audit gates show green checkmarks
- ‚úÖ Naturalness score is 8+/10
- ‚úÖ Word count meets or exceeds target
- ‚úÖ NO violations remain

**INCOMPLETE means:**
- ‚ùå ANY gate shows red X
- ‚ö†Ô∏è ANY gate shows warning
- Word count below target
- ANY violations remain

**If incomplete: KEEP WORKING. Loop until complete.**

## Input

- **Module file**: Complete module from Stages 1-3
- **Level**: Determines constraints and expectations

## Process

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 REVIEW MODULE                   ‚îÇ
‚îÇ         Run audit, check all constraints        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ    VIOLATIONS?        ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ         ‚îÇ
                   ‚îÇ NO      ‚îÇ YES
                   ‚ñº         ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   PASS!     ‚îÇ   ‚îÇ  COUNT VIOLATIONS       ‚îÇ
        ‚îÇ Output MDX  ‚îÇ   ‚îÇ  ‚â§3 = FIX               ‚îÇ
        ‚îÇ             ‚îÇ   ‚îÇ  >3 = REBUILD SECTION   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ   APPLY FIX or      ‚îÇ
                          ‚îÇ   REBUILD SECTION   ‚îÇ
                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
                              (loop back to REVIEW)
```

## Review Checklist

### 1. Template Compliance

- [ ] **Read the appropriate template** for this module type:
  - **B1 M01-05 (Metalanguage):** `docs/l2-uk-en/templates/b1-metalanguage-module-template.md`
  - **B1 M06-51 (Grammar):** `docs/l2-uk-en/templates/b1-grammar-module-template.md`
  - **B1 Checkpoints (M15, M25, M34, M41, M51 ‚Äî grammar phases only):** `docs/l2-uk-en/templates/b1-checkpoint-module-template.md`
  - **B1 M52-71 (Vocabulary):** `docs/l2-uk-en/templates/b1-vocab-module-template.md`
  - **B1 M72-81 (Cultural):** `docs/l2-uk-en/templates/b1-cultural-module-template.md`
  - **B1 M82-86 (Integration):** `docs/l2-uk-en/templates/b1-integration-module-template.md`
  - **B2:** `docs/l2-uk-en/templates/b2-module-template.md`
  - **C1:** `docs/l2-uk-en/templates/c1-module-template.md`
  - **C2:** `docs/l2-uk-en/templates/c2-module-template.md`
  - **LIT:** `docs/l2-uk-en/templates/lit-module-template.md`
- [ ] Module structure matches template sections
- [ ] Word count meets template minimum
- [ ] Activity count and types match template requirements
- [ ] Vocabulary count meets template specification

### 2. Structural Audit

- [ ] Metadata YAML sidecar exists and has required fields
- [ ] Vocabulary YAML sidecar exists (no embedded table)
- [ ] Activities YAML sidecar exists (no embedded activities)

### 3. Grammar Constraints

- [ ] Only uses grammar allowed at this level
- [ ] See `{LEVEL}-CURRICULUM-PLAN.md` –ö–∞—Ç–∞–ª–æ–≥ –í

### 4. Vocabulary Constraints

- [ ] Vocabulary YAML matches Schema (POS, Gender, IPA)
- [ ] Uses vocabulary from curriculum plan
- [ ] `validate_vocab_yaml.py` passes
- **Note:** Cross-module vocab validation deferred to pipeline

### 5. Activity Constraints

- [ ] Count meets minimum (8-16+ by level)
- [ ] Items per activity meets minimum (12-18+ by level)
- [ ] Type variety (4-5+ types)
- [ ] Correct syntax (fill-in `___`, unjumble `/`, etc.)
- [ ] All answers correct

### 6. Richness Constraints (Counts)

**CRITICAL: Read `docs/RICHNESS-SCORING-GUIDE.md` for scoring details and fix templates.**

- [ ] Word count meets target
- [ ] Example sentences meet minimum
- [ ] Engagement boxes meet minimum
- [ ] Mini-dialogues present

When richness fails, check the audit report for **Dryness Flags** and use the exact fix templates from the guide.

### 7. Content Richness Quality (B1+ Critical)

**This is not about counts. This is about whether the content is ALIVE or DEAD.**

Check each section for these quality indicators:

#### 7a. Engagement Quality

**DRY (robot wrote this):**

```markdown
–î–æ–∫–æ–Ω–∞–Ω–∏–π –≤–∏–¥ –ø–æ–∫–∞–∑—É—î –∑–∞–≤–µ—Ä—à–µ–Ω—É –¥—ñ—é.
–ù–µ–¥–æ–∫–æ–Ω–∞–Ω–∏–π –≤–∏–¥ –ø–æ–∫–∞–∑—É—î –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω—É –¥—ñ—é.
–î–∏–≤—ñ—Ç—å—Å—è —Ç–∞–±–ª–∏—Ü—é –Ω–∏–∂—á–µ.
```

**RICH (learner will remember this):**

```markdown
–£—è–≤—ñ—Ç—å: –≤–∏ —á–∏—Ç–∞—î—Ç–µ –∫–Ω–∏–≥—É –≤–µ—Å—å –≤–µ—á—ñ—Ä ‚Äî —Ü–µ –ø—Ä–æ—Ü–µ—Å, –Ω–µ–¥–æ–∫–æ–Ω–∞–Ω–∏–π –≤–∏–¥.
–ê–ª–µ –æ—Å—å –≤–∏ –∑–∞–∫—Ä–∏–ª–∏ –∫–Ω–∏–≥—É ‚Äî –≥–æ—Ç–æ–≤–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç. –î–æ–∫–æ–Ω–∞–Ω–∏–π –≤–∏–¥.

–¶–µ —è–∫ —Ä—ñ–∑–Ω–∏—Ü—è –º—ñ–∂ ¬´—è –π—à–æ–≤ –¥–æ–¥–æ–º—É¬ª (–º–æ–∂–µ, —â–µ –π–¥—É) —ñ ¬´—è –ø—Ä–∏–π—à–æ–≤¬ª (—Ç–æ—á–∫–∞, —Ñ—ñ–Ω—ñ—à).

üí° **–ß–æ–º—É —Ü–µ –≤–∞–∂–ª–∏–≤–æ?**
–£–∫—Ä–∞—ó–Ω—Ü—ñ —á—É—é—Ç—å —Ü—é —Ä—ñ–∑–Ω–∏—Ü—é –æ–¥—Ä–∞–∑—É. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –≤–∏–¥ ‚Äî
—ñ —Ä–µ—á–µ–Ω–Ω—è –∑–≤—É—á–∏—Ç—å... –¥–∏–≤–Ω–æ. –Ø–∫ —Ñ–∞–ª—å—à–∏–≤–∞ –Ω–æ—Ç–∞ –≤ –ø—ñ—Å–Ω—ñ.
```

#### 7b. Variety Check

**Count unique sentence starters in each section.** If >50% of sentences start the same way, flag as DRY.

‚ùå DRY pattern:

```markdown
–î–æ–∫–æ–Ω–∞–Ω–∏–π –≤–∏–¥ –æ–∑–Ω–∞—á–∞—î...
–î–æ–∫–æ–Ω–∞–Ω–∏–π –≤–∏–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è...
–î–æ–∫–æ–Ω–∞–Ω–∏–π –≤–∏–¥ –ø–æ–∫–∞–∑—É—î...
–î–æ–∫–æ–Ω–∞–Ω–∏–π –≤–∏–¥ –º–∞—î...
```

‚úÖ RICH pattern:

```markdown
–ö–æ–ª–∏ –¥—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ ‚Äî —Ü–µ –¥–æ–∫–æ–Ω–∞–Ω–∏–π –≤–∏–¥.
–£–∫—Ä–∞—ó–Ω—Ü—ñ –∫–∞–∂—É—Ç—å ¬´—è –ø—Ä–æ—á–∏—Ç–∞–≤ –∫–Ω–∏–≥—É¬ª, –±–æ –∫–Ω–∏–≥–∞ –∑–∞–∫—ñ–Ω—á–µ–Ω–∞.
–ê —è–∫—â–æ —â–µ —á–∏—Ç–∞—é? –¢–æ–¥—ñ ¬´—á–∏—Ç–∞—é¬ª ‚Äî –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É.
–ü–æ—Ä—ñ–≤–Ω—è–π—Ç–µ: ¬´–≤—ñ–Ω –ø–∏—Å–∞–≤ –ª–∏—Å—Ç¬ª vs ¬´–≤—ñ–Ω –Ω–∞–ø–∏—Å–∞–≤ –ª–∏—Å—Ç¬ª.
```

#### 7c. Emotional Hooks

**Each major section needs at least one of:**

- Metaphor or analogy (—è–∫ —Ñ–∞–ª—å—à–∏–≤–∞ –Ω–æ—Ç–∞, —è–∫ —Ä—ñ–∑–Ω–∏—Ü—è –º—ñ–∂ X —ñ Y)
- Real-world scenario (—É—è–≤—ñ—Ç—å: –≤–∏ –Ω–∞ —Å–ø—ñ–≤–±–µ—Å—ñ–¥—ñ...)
- Cultural connection (—É–∫—Ä–∞—ó–Ω—Ü—ñ –∫–∞–∂—É—Ç—å —Ç–∞–∫, –±–æ...)
- Surprise or contrast (–∞–ª–µ —Ç—É—Ç —î —Å—é—Ä–ø—Ä–∏–∑!)
- Question to reader (–∞ —â–æ —è–∫—â–æ...? —á–æ–º—É —Ç–∞–∫?)

‚ùå No hooks = textbook voice = learner falls asleep

‚úÖ Has hooks = conversation voice = learner stays engaged

#### 7d. Cultural Depth (B1+)

**Each module should include:**

- [ ] At least 1 named Ukrainian place (–õ—å–≤—ñ–≤, –ö–∞—Ä–ø–∞—Ç–∏, –î–Ω—ñ–ø—Ä–æ)
- [ ] At least 1 cultural reference (traditional, historical, or contemporary)
- [ ] Real-world context showing WHY this grammar/vocab matters

‚ùå Generic: "–õ—é–¥–∏–Ω–∞ –∫—É–ø—É—î —Ö–ª—ñ–± —É –º–∞–≥–∞–∑–∏–Ω—ñ."
‚úÖ Specific: "–û–∫—Å–∞–Ω–∞ –∫—É–ø—É—î –ø–∞–ª—è–Ω–∏—Ü—é –Ω–∞ –ë–µ—Å–∞—Ä–∞–±—Å—å–∫–æ–º—É —Ä–∏–Ω–∫—É –≤ –ö–∏—î–≤—ñ."

#### 7e. Proverbs & Idioms (B1+)

**Each grammar module should include 1-2 proverbs or idioms that:**

- Naturally demonstrate the grammar point
- Are woven into content, not just listed
- Have cultural context explained

Example for aspect:

```markdown
–£–∫—Ä–∞—ó–Ω—Ü—ñ –∫–∞–∂—É—Ç—å: ¬´–ù–µ –∫–∞–∂–∏ –≥–æ–ø, –ø–æ–∫–∏ –Ω–µ –ø–µ—Ä–µ—Å–∫–æ—á–∏—à¬ª.
–ó–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É: **–ø–µ—Ä–µ—Å–∫–æ—á–∏—à** ‚Äî –¥–æ–∫–æ–Ω–∞–Ω–∏–π –≤–∏–¥.
–ß–æ–º—É? –ë–æ –π–¥–µ—Ç—å—Å—è –ø—Ä–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –ø–µ—Ä–µ—Å—Ç—Ä–∏–±–Ω—É–≤ —á–∏ –Ω—ñ.
```

#### 7f. Richness Score Calculation

For each section, mentally score:

| Criterion       | 0                   | 1                | 2                         |
| --------------- | ------------------- | ---------------- | ------------------------- |
| Engagement      | Textbook voice      | Some personality | Conversational, memorable |
| Variety         | Repetitive starters | Mixed            | Varied, rhythmic          |
| Hooks           | None                | 1-2              | 3+ per section            |
| Cultural depth  | Generic examples    | Some specifics   | Rich, placed content      |
| Proverbs/idioms | None                | 1 (forced)       | 1-2 (natural)             |

**Total 0-4:** ‚ùå REWRITE section
**Total 5-7:** ‚ö†Ô∏è ENRICH section
**Total 8-10:** ‚úÖ PASS

#### 7g. Quick Dryness Flags

Flag content as DRY if ANY of these are true:

| Flag                  | Pattern                                                   |
| --------------------- | --------------------------------------------------------- |
| TEXTBOOK_VOICE        | No questions, metaphors, or emotional hooks in 300+ words |
| REPETITIVE            | Same sentence structure >5 times in section               |
| GENERIC_EXAMPLES      | No named people, places, or specific scenarios            |
| LIST_DUMP             | Explanation is just a list without narrative flow         |
| NO_CULTURAL_ANCHOR    | Grammar taught without Ukrainian cultural context         |
| ENGAGEMENT_BOX_FILLER | üí° boxes just restate what was already said               |

**If 2+ flags: Section needs REWRITE, not just fix.**

### 8. Linguistic Purity

- [ ] No Surzhyk or "Ghost Words" (Verify spelling is Ukrainian, not Russian). See LINGUISTIC-PURITY-GUIDE.md
- [ ] No AI contamination ("wait", "actually", "let me")
- [ ] Correct Ukrainian spelling and grammar
- [ ] **NO Russian Characters**: Search for `—ë`, `—ä`, `—ã`, `—ç` (Forbidden).
- [ ] **NO Russian Phonetics**: No comparisons like "Ukrainian –ò is like Russian –´".

### 9. Naturalness Check

After grammar and vocabulary validation, check ALL Ukrainian text for naturalness.

**Purpose:** Prevent disconnected drills, template repetition, and robotic flow that grammar/vocabulary checks miss.

**How it works:** You evaluate naturalness directly using your Ukrainian language knowledge. No external tools or MCP servers required. As an LLM trained on extensive Ukrainian text, you can assess flow, register, and authenticity.

**CRITICAL:** Naturalness is NEVER "N/A" - every module with Ukrainian text requires evaluation, including alphabet modules with simple instructions.

#### 9.1 Extract ALL Ukrainian Text

Evaluate ALL Ukrainian content in the module:

**Always evaluate:**
- **Activity instructions** (e.g., "–ó'—î–¥–Ω–∞–π—Ç–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ –µ–ª–µ–º–µ–Ω—Ç–∏", "–û–±–µ—Ä—ñ—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å")
- **Cloze passages** (any length)
- **Fill-in sentences/paragraphs**
- **Unjumble sentences**
- **Quiz explanations** in Ukrainian
- **Mark-the-words text passages**

**Minimal evaluation (just check instructions):**
- `match-up`, `group-sort` - typically just have instruction text
- `quiz`, `true-false`, `select` - check instruction + any Ukrainian explanations

#### 9.2 Analyze Naturalness (Switch to Ukrainian Language Mode)

Score each prose activity 1-10 based on:

1. **Subject consistency** - Are subjects maintained throughout passages?
2. **Discourse markers** - Presence of connectors (–∞, –∞–ª–µ, –ø–æ—Ç—ñ–º, —Ç–æ–º—É, —Ç–∞–∫–æ–∂, —Å–ø–æ—á–∞—Ç–∫—É, –Ω–∞—Ä–µ—à—Ç—ñ)
3. **Topic coherence** - Do passages maintain unified topics or jump randomly?
4. **Redundancy** - Are there repetitive patterns or disconnected sentences?

**Red flags (score < 8/10):**

| Issue | Example |
|-------|---------|
| **Template repetition** | Same sentence structure repeated across multiple activities in module |
| **Excessive intensifiers** | "–¥—É–∂–µ" used 5+ times, or "–Ω–∞–¥–∑–≤–∏—á–∞–π–Ω–æ/—Å–ø—Ä–∞–≤–∂–Ω—ñ–π" overused |
| **Double superlatives** | "–Ω–∞–π–≤–∏–¥–∞—Ç–Ω—ñ—à–∏–π —Ç–∞ –Ω–∞–π–≤—ñ–¥–æ–º—ñ—à–∏–π" (semantically redundant) |
| **Missing discourse markers** | List of disconnected factoids with no connectors |
| **Robotic transitions** | "—ñ —Ü–µ –¥–æ–ø–æ–º–∞–≥–∞—î...", "—Ç–æ–º—É —â–æ... —Ç–æ–º—É" (mechanical constructions) |

#### 9.3 Scoring Standards

| Module Type | Target Score | Flag If |
|-------------|--------------|---------|
| **Content modules** | 8/10 | < 8/10 |
| **Checkpoints/Review** | 7/10 | < 7/10 |
| **Minimal prose modules** | 8/10 | < 8/10 (still evaluate instructions) |

**Score all Ukrainian text** in the module. If average is below target, flag for fixes.

#### 9.3.1 Update Meta File After Scoring

After evaluating naturalness, **always update the module's meta file**:

```yaml
# curriculum/l2-uk-en/{level}/meta/{num}-{slug}.yaml
naturalness:
  score: 9      # Your evaluated score (1-10)
  status: PASS  # PASS if score >= 8 (or >= 7 for checkpoints), else PENDING/FAIL
```

**Status values:**
- `PASS` - Score meets threshold, audit will pass
- `PENDING` - Not yet evaluated (causes audit FAIL)
- `FAIL` - Evaluated but below threshold

The audit reads this meta file and will FAIL if status is not PASS or score < 8.

#### 9.4 Fix Flagged Issues

If module average score < target:

1. **Identify specific patterns** causing issues (which red flags apply?)
2. **Propose fixes** using ONLY:
   - Vocabulary from M01-M{current} (check cumulative vocab)
   - Grammar from curriculum plan (check allowed constructs)
3. **Apply fixes** to activities YAML file
4. **Re-score** to verify improvement

**Common fix strategies:**

| Issue | Fix |
|-------|-----|
| Template repetition | Vary sentence structures across activities |
| Excessive intensifiers | Remove 50% of "–¥—É–∂–µ", eliminate "–Ω–∞–¥–∑–≤–∏—á–∞–π–Ω–æ/—Å–ø—Ä–∞–≤–∂–Ω—ñ–π" unless essential |
| Double superlatives | Replace with single precise descriptor |
| Missing discourse markers | Add 2-3 connectors per 10-sentence passage (—Ç–∞–∫–æ–∂, –ø—Ä–æ—Ç–µ, —Ç–æ–º—É, —Å–ø–æ—á–∞—Ç–∫—É) |
| Robotic transitions | Simplify mechanical constructions, use natural flow |

**See also:** For batch naturalness scanning of completed modules, use `/scan-naturalness {level} {start} {end}`

### 10. Seminar Track Pairing (LIT, B2-HIST, C1-HIST, C1-BIO)

**Applies to:** LIT, B2-HIST, C1-HIST, C1-BIO tracks only.

Seminar tracks use **Reading-Analysis Pairs** architecture:
- Every analytical activity MUST link to a reading source
- `reading` activities provide INPUT (source text)
- `essay-response`, `critical-analysis`, `comparative-study` provide OUTPUT (analysis)

#### 10.1 Audit Violations

| Violation | Severity | Meaning |
|-----------|----------|---------|
| `READING_MISSING_ID` | CRITICAL | Reading activity lacks `id` field |
| `MISSING_SOURCE_READING` | CRITICAL | Analytical activity lacks `source_reading` link |
| `INVALID_SOURCE_READING` | CRITICAL | `source_reading` points to non-existent reading id |
| `ORPHAN_READING` | WARNING | Reading defined but never referenced |

#### 10.2 Fix: Add Missing Reading Activity

If module has analytical activities but no `reading` activity:

**Step 1:** Identify source material from the module content (prose section)

**Step 2:** Create reading activity at the TOP of the YAML:

```yaml
- type: reading
  id: reading-01                    # REQUIRED: unique ID
  title: '–î–∂–µ—Ä–µ–ª–æ: [Topic]'
  source: '[Author] ([Year])'       # Attribution
  text: |
    [Extract 200-500 words of primary source text
    that the analytical activities will analyze]
```

**Step 3:** Link existing analytical activities:

```yaml
- type: essay-response
  source_reading: reading-01        # ADD THIS LINE
  title: '–ï—Å–µ: ...'
  prompt: '...'
```

#### 10.3 Fix: Add source_reading to Existing Activities

For each `essay-response`, `critical-analysis`, `comparative-study`, or `authorial-intent`:

1. Identify which reading it analyzes
2. Add `source_reading: reading-XX` field
3. Ensure the referenced reading exists

**Example fix:**

```yaml
# BEFORE (fails audit)
- type: critical-analysis
  title: '–ê–Ω–∞–ª—ñ–∑ —Å–∏–º–≤–æ–ª—ñ–∫–∏'
  target_text: '–ü–æ—Ö–æ–≤–∞–π—Ç–µ —Ç–∞ –≤—Å—Ç–∞–≤–∞–π—Ç–µ...'
  questions:
    - '–Ø–∫—É —Ñ—É–Ω–∫—Ü—ñ—é –≤–∏–∫–æ–Ω—É—î —ñ–º–ø–µ—Ä–∞—Ç–∏–≤?'

# AFTER (passes audit)
- type: critical-analysis
  source_reading: reading-01        # ‚Üê ADDED
  title: '–ê–Ω–∞–ª—ñ–∑ —Å–∏–º–≤–æ–ª—ñ–∫–∏'
  target_text: '–ü–æ—Ö–æ–≤–∞–π—Ç–µ —Ç–∞ –≤—Å—Ç–∞–≤–∞–π—Ç–µ...'
  questions:
    - '–Ø–∫—É —Ñ—É–Ω–∫—Ü—ñ—é –≤–∏–∫–æ–Ω—É—î —ñ–º–ø–µ—Ä–∞—Ç–∏–≤?'
```

#### 10.4 Fix: Reading Missing ID

```yaml
# BEFORE (fails audit)
- type: reading
  title: '–î–∂–µ—Ä–µ–ª–æ: –ó–∞–ø–æ–≤—ñ—Ç'
  text: |
    –Ø–∫ —É–º—Ä—É, —Ç–æ –ø–æ—Ö–æ–≤–∞–π—Ç–µ...

# AFTER (passes audit)
- type: reading
  id: reading-testament              # ‚Üê ADDED (pattern: reading-[slug])
  title: '–î–∂–µ—Ä–µ–ª–æ: –ó–∞–ø–æ–≤—ñ—Ç'
  text: |
    –Ø–∫ —É–º—Ä—É, —Ç–æ –ø–æ—Ö–æ–≤–∞–π—Ç–µ...
```

#### 10.5 Multiple Readings Strategy

For modules with multiple source texts:

```yaml
# First reading for first set of analyses
- type: reading
  id: reading-01
  title: '–î–∂–µ—Ä–µ–ª–æ 1: ...'
  text: '...'

# Second reading for comparison
- type: reading
  id: reading-02
  title: '–î–∂–µ—Ä–µ–ª–æ 2: ...'
  text: '...'

# Analytical activity referencing first reading
- type: essay-response
  source_reading: reading-01
  title: '–ï—Å–µ –ø—Ä–æ –¥–∂–µ—Ä–µ–ª–æ 1'

# Comparative study using both
- type: comparative-study
  source_reading: reading-01        # Primary reference
  title: '–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –¥–≤–æ—Ö –¥–∂–µ—Ä–µ–ª'
  items_to_compare:
    - '–î–∂–µ—Ä–µ–ª–æ 1'
    - '–î–∂–µ—Ä–µ–ª–æ 2'
```

#### 10.6 Quick Fix Checklist for Seminar Tracks

- [ ] At least ONE `reading` activity exists
- [ ] ALL `reading` activities have `id` field
- [ ] ALL `essay-response` have `source_reading`
- [ ] ALL `critical-analysis` have `source_reading`
- [ ] ALL `comparative-study` have `source_reading`
- [ ] ALL `authorial-intent` have `source_reading`
- [ ] ALL `source_reading` values point to valid reading IDs

#### 10.7 LLM Self-Validation (MANDATORY for Audit)

<critical>

**SKEPTICAL VALIDATION - NOT A RUBBER STAMP**

You are the last line of defense. The audit script catches syntax and structure, but YOU must catch:
- Wrong URLs pointing to wrong authors
- Factual errors in historical content
- Unnatural or robotic Ukrainian
- Model answers that don't match prompts

**Approach:**
- **Assume errors exist** until you verify otherwise
- **Actively look for problems**, don't just confirm correctness
- **Check specific details** (dates, names, URLs) against your knowledge
- **If uncertain, FLAG IT** - false positives are better than missed errors

**DO NOT:**
- ‚ùå Skim and approve
- ‚ùå Copy-paste "all checks passed" without verification
- ‚ùå Assume previous creator got it right

**DO:**
- ‚úÖ Read every Ukrainian sentence critically
- ‚úÖ Verify each URL against known mappings
- ‚úÖ Check that model answers actually address prompts
- ‚úÖ Document specific evidence for each check

---

**The audit script cannot verify content accuracy. YOU (the LLM doing the audit) MUST verify these against your knowledge:**

**1. External URL Verification**

For every `reading` activity with a `resource.url`:

- Do you recognize this URL from your training data?
- Does the URL content match the expected author/topic?
- Known URL mappings (verify or flag):

| Author | UkrLib URL |
|--------|------------|
| –ù–µ—á—É–π-–õ–µ–≤–∏—Ü—å–∫–∏–π | `tid=1646` |
| –®–µ–≤—á–µ–Ω–∫–æ | `tid=57` |
| –ö—É–ª—ñ—à | `tid=1621` |
| –ö–æ—Ç–ª—è—Ä–µ–≤—Å—å–∫–∏–π | `tid=1553` |
| –§—Ä–∞–Ω–∫–æ | `tid=71` |
| –õ–µ—Å—è –£–∫—Ä–∞—ó–Ω–∫–∞ | `tid=83` |
| –ö–æ–±–∏–ª—è–Ω—Å—å–∫–∞ | `tid=1582` |
| –ö–≤—ñ—Ç–∫–∞-–û—Å–Ω–æ–≤'—è–Ω–µ–Ω–∫–æ | `tid=1568` |

**If URL doesn't match your knowledge: FLAG as `INVALID_EXTERNAL_URL` with suggested correction.**

**2. Reading-Analysis Coherence**

For every `critical-analysis` and `authorial-intent`:

- Does `target_text` actually appear in or derive from the source reading?
- Do the `questions` relate directly to the reading content?
- Are `focus_points` appropriate for the target text?

**If mismatched: FLAG as `INCOHERENT_ANALYSIS` with explanation.**

**3. Model Answer Quality**

For every `essay-response`, `critical-analysis`, and `comparative-study`:

- Does `model_answer` actually address the prompt/questions?
- Is the analysis substantive and correct?
- Does it demonstrate the expected analytical approach?

**If model answer is off-topic or incorrect: FLAG as `INVALID_MODEL_ANSWER`.**

**4. Factual Accuracy**

For all historical/biographical content:

- Are dates correct?
- Are names spelled correctly?
- Are historical facts accurate?
- Are literary attributions correct?

**If factual error found: FLAG as `FACTUAL_ERROR` with correction.**

**Audit Report Format for Self-Validation Issues:**

```
=== LLM SELF-VALIDATION ===

INVALID_EXTERNAL_URL (CRITICAL)
  Activity: –ë—ñ–æ–≥—Ä–∞—Ñ—ñ—è –ù–µ—á—É—è-–õ–µ–≤–∏—Ü—å–∫–æ–≥–æ
  Current URL: https://www.ukrlib.com.ua/bio/printit.php?tid=1815
  Expected: tid=1646 (–ù–µ—á—É–π-–õ–µ–≤–∏—Ü—å–∫–∏–π)
  Reason: URL points to wrong author

FACTUAL_ERROR (WARNING)
  Activity: –ï—Å–µ –ø—Ä–æ —Ç–≤–æ—Ä—á—ñ—Å—Ç—å
  Issue: "–Ω–∞—Ä–æ–¥–∏–≤—Å—è 1845" should be "–Ω–∞—Ä–æ–¥–∏–≤—Å—è 1838"
```

**Logging Results (MANDATORY):**

Write self-validation results to a **separate file** (audit.py overwrites the main review file):

**File:** `curriculum/l2-uk-en/{level}/audit/{slug}-llm-review.md`

```markdown
# LLM Self-Validation: {slug}
**Validated by:** Claude/Gemini | **Date:** YYYY-MM-DD
**Content Hash:** {first 8 chars of md5 hash of module .md file}

## Verification Evidence

### External URLs
**Status:** ‚úÖ/‚ùå
- URL: `{actual URL from module}`
- Expected author: {author name}
- Verified against: {tid mapping or knowledge source}
- Evidence: "{quote from page title or content proving correct author}"

### Reading-Analysis Coherence
**Status:** ‚úÖ/‚ùå
- Reading ID: `{reading-id}`
- Linked activities: {list of activity types referencing it}
- Target text verification: "{first 10 words of target_text}" appears in source: YES/NO

### Model Answers
**Status:** ‚úÖ/‚ùå
- Checked {N} model answers
- Each addresses its prompt: YES/NO
- Quality issues: {specific issues or "None found"}

### Factual Accuracy
**Status:** ‚úÖ/‚ùå
- Key facts verified:
  - Birth/death dates: {dates} ‚úì
  - Locations: {places} ‚úì
  - Historical events: {events} ‚úì
- Corrections needed: {list or "None"}

### Naturalness
**Status:** ‚úÖ/‚ùå | **Score:** X/10
- Checked {N} prose passages
- Red flags: {template repetition, robotic transitions, etc. or "None"}

## Issues Found
[MUST list specific issues with line numbers/activity names, or explicitly state "None found after checking X items"]

## Fixes Applied
[List specific fixes with before/after, or "None needed"]
```

**EVIDENCE REQUIRED:** Generic statements like "all checks passed" are NOT acceptable. Each section must show WHAT was checked and HOW it was verified.

**Content Hash:** Run `md5 -q {module}.md | cut -c1-8` to get the hash. If module changes after review, audit will FAIL as stale.

**Two-file structure:**
- `{slug}-review.md` - Auto-generated by `audit.py` (script output)
- `{slug}-llm-review.md` - Written by LLM (self-validation, never overwritten by script)

</critical>

### 11. Core Module LLM Self-Validation (A1-C2)

**Applies to:** A1, A2, B1, B2, C1, C2 core levels.

<critical>

**SKEPTICAL VALIDATION - NOT A RUBBER STAMP**

You are the last line of defense for core modules. Be aggressive in finding errors:
- Russianisms slip through constantly (–ø—Ä–∏–π–º–∞—Ç–∏ —É—á–∞—Å—Ç—å, –Ω–∞ –ø—Ä–æ—Ç—è–∑—ñ)
- English calques from translation (—Ä–æ–±–∏—Ç–∏ —Å–µ–Ω—Å, –±—Ä–∞—Ç–∏ –º—ñ—Å—Ü–µ)
- Wrong case endings in activities
- Vocabulary too advanced for the level

**Approach:**
- **Read every Ukrainian sentence** - don't skim
- **Check vocabulary against curriculum plan** - is this word allowed at this level?
- **Verify activity answers** - are the "correct" answers actually correct?
- **Flag uncertainty** - if you're not sure, mark it for review

---

**The audit script checks structure and counts but cannot verify content accuracy. YOU (the LLM doing the audit) MUST verify:**

**1. Ukrainian Grammar Correctness**

- Are all Ukrainian sentences grammatically correct?
- Are case endings correct for the context?
- Is word order natural (not calqued from English)?
- Are verb aspects used correctly?

**Common errors to check:**
| ‚ùå Wrong | ‚úÖ Correct | Issue |
|----------|-----------|-------|
| –ø—Ä–∏–π–º–∞—Ç–∏ —É—á–∞—Å—Ç—å | –±—Ä–∞—Ç–∏ —É—á–∞—Å—Ç—å | Russicism |
| —Å–∞–º–∏–π –∫—Ä–∞—â–∏–π | –Ω–∞–π–∫—Ä–∞—â–∏–π | Superlative calque |
| –Ω–∞ –ø—Ä–æ—Ç—è–∑—ñ | –ø—Ä–æ—Ç—è–≥–æ–º | Russicism |
| —Ä–æ–±–∏—Ç–∏ —Å–µ–Ω—Å | –º–∞—Ç–∏ —Å–µ–Ω—Å | English calque |

**2. Vocabulary Appropriateness**

- Is vocabulary within the level scope? (Check curriculum plan)
- Are words used in correct contexts?
- Are collocations natural?

**3. Activity Instructions Clarity**

- Are instructions unambiguous?
- Can learners understand what to do?
- Are example answers correct?

**4. Cultural/Factual Accuracy**

- Are cultural references accurate?
- Are any facts stated that could be wrong?

**Logging Results (MANDATORY):**

**File:** `curriculum/l2-uk-en/{level}/audit/{slug}-llm-review.md`

```markdown
# LLM Self-Validation: {slug}
**Validated by:** Claude/Gemini | **Date:** YYYY-MM-DD
**Content Hash:** {first 8 chars of md5 hash of module .md file}

## Verification Evidence

### Grammar
**Status:** ‚úÖ/‚ùå
- Sentences checked: {N}
- Russianisms found: {list or "None"}
- English calques found: {list or "None"}
- Case/aspect errors: {list or "None"}
- Sample verified: "{quote a sentence you checked}"

### Vocabulary
**Status:** ‚úÖ/‚ùå
- Level: {A1/A2/B1/B2/C1/C2}
- Words checked against curriculum plan: {N}
- Out-of-scope words found: {list or "None"}
- Collocation issues: {list or "None"}

### Activity Instructions
**Status:** ‚úÖ/‚ùå
- Activities reviewed: {N}
- Ambiguous instructions: {list with activity names or "None"}
- Incorrect example answers: {list or "None"}

### Factual Accuracy
**Status:** ‚úÖ/‚ùå
- Cultural references verified: {list what you checked}
- Corrections needed: {list or "None"}

### Naturalness
**Status:** ‚úÖ/‚ùå | **Score:** X/10
- Prose passages checked: {N}
- Red flags: {specific issues or "None"}

## Issues Found
[MUST list specific issues with line numbers/activity names, or explicitly state "None found after checking X items"]

## Fixes Applied
[List specific fixes with before/after, or "None needed"]
```

**EVIDENCE REQUIRED:** Generic statements like "all checks passed" are NOT acceptable. Each section must show WHAT was checked and HOW it was verified.

**Content Hash:** Run `md5 -q {module}.md | cut -c1-8` to get the hash. If module changes after review, audit will FAIL as stale.

</critical>

---

## ‚ö° Specialized Fix Protocols

### Common YAML Schema Violations - Quick Reference

**Use this table to diagnose and fix schema errors immediately:**

**For minimum item counts:** Check `claude_extensions/quick-ref/{level}.md` (each level has different minimums)

| Error Pattern | Root Cause | Fix |
|--------------|------------|-----|
| Quiz/fill-in/true-false has too few items | Below level-specific minimum | Check quick-ref for your level's minimum, add items |
| Cloze has too few blanks | Below level-specific minimum | Check quick-ref for your level's minimum, add blanks |
| Fill-in: "missing required 'options'" | `options` field required for each item | Add `options: [opt1, opt2, opt3, opt4]` |
| Cloze: "not valid under any schema" | Has both `blanks:` array AND inline `{a\|b}` | Remove `blanks:` array OR switch to numbered format |
| "Field 'instructions' not recognized" | Schema requires singular | Change to `instruction:` (no 's') |
| "Field 'id' not allowed" | `additionalProperties: false` | Remove `id:` field (only for LIT activities) |
| "Unexpected character near apostrophe" | Ukrainian apostrophe in single quotes | Change to double quotes: `"—ñ–Ω—Ç–µ—Ä–≤'—é"` |
| "Activity type 'writing' invalid" | Wrong type name | Use `essay-response` instead |
| Unjumble: "array too short" | Less than 6 words | Add words to reach 6+ per sentence |

### Fix Protocol: YAML Parse Errors

**If audit shows "Error parsing YAML":**

1. **Check for apostrophe conflicts:**
   ```bash
   grep -n "answer: '" activities/file.yaml
   ```
   Fix: Change all single-quoted strings with apostrophes to double quotes

2. **Check for unquoted colons:**
   ```bash
   grep -n ": " activities/file.yaml | grep -v "^  *-"
   ```
   Fix: Quote any string values containing colons

3. **Validate YAML syntax:**
   ```bash
   .venv/bin/python -c "import yaml; yaml.safe_load(open('activities/file.yaml'))"
   ```

### Fix Protocol: Schema Validation Errors

**Step-by-step diagnostic:**

1. **Identify activity index** from error: `Schema validation error at key '9'`
2. **Count activities** in YAML (0-indexed, so key '9' = 10th activity)
3. **Check activity type** and **compare to schema requirements:**
   - Read `schemas/activities-{level}.schema.json` for your level (e.g., `activities-b2.schema.json`)
   - Check `minItems`, `required` fields, `additionalProperties`
   - Cross-reference with `claude_extensions/quick-ref/{level}.md` for minimums

4. **Common fixes:**
   - **Missing fields:** Add required fields (`title`, `instruction`, `items`)
   - **Extra fields:** Remove fields not in schema (`id`, `instructions` with 's')
   - **Wrong minimum:** Add items to meet level minimums (see quick-ref)
   - **Wrong structure:** Fix nested object format (e.g., `options` array must have exactly 4 items)

### Fix Protocol: Stale LLM Review

```bash
# Calculate new content hash
.venv/bin/python << 'EOF'
import sys, hashlib
sys.path.append('scripts')
from audit.core import extract_core_content, clean_for_stats

content = open('curriculum/l2-uk-en/{level}/{file}.md').read()
core = extract_core_content(content)
prose = clean_for_stats(core)
new_hash = hashlib.md5(prose.encode('utf-8')).hexdigest()[:8]
print(f"New hash: {new_hash}")
EOF

# Update hash in LLM review file
# Then run audit again to verify PASS
```

**CRITICAL:** After updating hash, run audit immediately to ensure review is now valid.

---

## Fix Strategy

### Minor Violations (‚â§3 issues)

Apply targeted fixes:

- Missing vocabulary ‚Üí Add to table
- Wrong syntax ‚Üí Correct the specific line
- Missing engagement box ‚Üí Add one
- Spelling error ‚Üí Fix it

### Major Violations (>3 issues in same section)

Rebuild the section:

- Content section failing ‚Üí Rewrite entire section
- Multiple activity failures ‚Üí Delete all activities, recreate
- Grammar violations throughout ‚Üí Rewrite affected paragraphs

### Catastrophic (>10 violations OR structural issues)

Rebuild from Stage 1:

- Frontmatter wrong ‚Üí Start over
- Wrong pedagogy structure ‚Üí Start over
- Vocabulary fundamentally wrong ‚Üí Start over

## Running the Audit

```bash
# Auto-fix YAML schema violations, then run audit
.venv/bin/python scripts/audit_module.py {file_path} --fix
```

**The `--fix` flag automatically fixes common YAML schema violations:**

- Removes invalid `id` properties
- Extracts `correct_words` from mark-the-words passage
- Converts `scrambled` to `words` array in unjumble
- Adds missing `source` in translate
- Renames `prompt`/`text` to `question` in quiz/select

**Workflow:**

1. Auto-fix runs FIRST (if violations found)
2. Audit runs AFTER fixes applied
3. If audit still fails, manual fixes needed

Audit output categories:

- **FAIL**: Must fix (grammar, vocabulary, syntax)
- **WARN**: Should fix (richness, variety)
- **INFO**: Optional improvement

## Iteration Limit

Maximum 3 fix iterations per stage. If still failing after 3:

1. Report the persistent issues
2. Ask user for guidance
3. Consider rebuilding from earlier stage

## Output on PASS

When audit passes, run the full pipeline:

```bash
# Full pipeline: lint ‚Üí generate MDX ‚Üí validate MDX ‚Üí validate HTML
npm run pipeline l2-uk-en {level} {module_num}
```

The pipeline validates:

1. **Lint**: MD format compliance
2. **Generate**: Creates MDX for Docusaurus
3. **Validate MDX**: Ensures no content loss during conversion
4. **Validate HTML**: Headless browser check for rendering errors

**Note:** HTML validation requires dev server running (`cd docusaurus && pnpm start`)

Report:

- Final audit score
- Pipeline status (PASS/FAIL)
- MDX file location
- "MODULE APPROVED"
